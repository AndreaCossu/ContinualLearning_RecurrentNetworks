exp_type: quickdraw
num_runs: 5
max_cpus: 30
cuda: true
max_gpus: 1 # used only if cuda is true
gpus_per_job: 0.33
cpus_per_job: 3

not_ray: false

epochs: 2
models: lstm
result_folder: /data/cossu/experiments/rnnpaper/multihead/quickdraw/lstm/agem_taskvectoronlytrain
dataroot: /data/cossu/quickdraw

# TASK CONFIGURATION
#grid_file: /home/cossu/continual-learning/CONFIGS/multihead/grid_quickdraw_agem.yaml
grid_file: ''
n_tasks: 10
n_tasks_val: 3 # -1 to do validation on all tasks
n_tasks_test: 10 # -1 to test on all tasks
output_size: 30
input_size: 3
max_label_value: 30 # if real value is greater, a % op will be applied on it.

# OPTIMIZER
weight_decay: 0
learning_rate: 1e-2
batch_size: 10
clip_grad: 5.0
use_sgd: true

# EXTRAS
multitask: false
multihead: true
not_test: false
not_intermediate_test: true
monitor: true # monitor metrics
save: true # save models
load: false # load previous models

# MODELS
expand_output: 0 #Expand output layer dynamically

# RNN - LSTM - LMN
hidden_size_rnn: 512
layers_rnn: 2
hidden_size_lmn: 128 # hidden dimension of functional component of LMN
memory_size_lmn: 128 # memory size of LMN
functional_out: true # compute output from functional instead of memory')
orthogonal: false # Use orthogonal recurrent matrixes
bidirectional: false

# MLP
hidden_sizes_mlp: # number of hidden units for each layer
 - 1024
relu_mlp: true # tanh if false

# LWTA
units_per_block: # number of units per block for each hidden layer
  - 3
blocks_per_layer: # number of blocks for each hidden layer
 - 10
activation_lwta: relu # relu, tanh, none

# ESN
reservoir_size: 128
spectral_radius: 0.9
sparsity: 0.0
alpha: 1.0
orthogonal_esn: false # Using orthogonal reservoir

# CNN
feed_conv_layers: # units for each feedforward layers of CNN
 - 256
 - 128
n_conv_layers: 3 # number of convolutional layers

classes_per_task: 2

# CL STRATEGIES
ewc_lambda: 0

mas_lambda: 0

cwr: false
ar1: false

agem: true
gem_patterns_per_step: 64
agem_sample_size: 512
gem: false
gem_memory_strength: 0
task_vector_at_test: true

lwf: false
lwf_temp: 1 # softmax temperature for LwF
warmup_epochs: 0 # how many warmup epochs for lwf

si_lambda: 0
eps: 1e-3 # SI epsilon

rehe_patterns: 0 # Number of rehearsal patterns per class
patterns_per_class_per_batch: 2 # can be -1 (disable this option, not rehearsal), 0 (add a minibatch equally split among classes) or >0 (specify patterns per class per minibatch)

truncated_time: 0 # Truncated time when computing importance gradient
